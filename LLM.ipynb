{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiolDxDzJvph"
   },
   "source": [
    "# **Large Language Model for NLP** ***Tasks***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elLHEo6GKK1E"
   },
   "source": [
    "## **1. Text Completion**\n",
    "\n",
    "<img src=\"https://cdn.pixabay.com/photo/2016/11/23/18/14/fountain-pen-1854169_960_720.jpg\" width=700px>\n",
    "\n",
    "Create a simple function that takes a sentence or phrase as input, and uses the language model to generate the next word or words to complete the sentence. The model can generate text that is grammatically correct and contextually relevant.\n",
    "\n",
    "**Business Context**\n",
    "\n",
    "- In content creation industries such as publishing, advertising, and media, text completion tools powered by LLMs allow for faster content generation by completing unfinished drafts, auto-suggesting paragraphs, or generating storylines. This enables creative professionals to focus on higher-level conceptual work while the LLM automates repetitive writing tasks.\n",
    "\n",
    "- In legal and contract management, text completion can be used to draft standard legal documents, contracts, or proposals by completing clauses or terms based on previous examples, significantly improving legal teams' productivity. Additionally, this tool is helpful in code completion for developers in software engineering, where it enhances development speed and accuracy by autocompleting code snippets based on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 50412,
     "status": "ok",
     "timestamp": 1769847836496,
     "user": {
      "displayName": "Ei Mon San",
      "userId": "14838997319009891563"
     },
     "user_tz": -660
    },
    "id": "not2lBxyGnjr",
    "outputId": "48825f45-736a-48d3-85d6-613082bb329d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.42.4\n",
      "  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (0.36.0)\n",
      "Collecting numpy<2.0,>=1.17 (from transformers==4.42.4)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (0.7.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.42.4)\n",
      "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.42.4) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.4) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.4) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.4) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.42.4) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.42.4) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.42.4) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.42.4) (2026.1.4)\n",
      "Downloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, tokenizers, transformers\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.2\n",
      "    Uninstalling tokenizers-0.22.2:\n",
      "      Successfully uninstalled tokenizers-0.22.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.6\n",
      "    Uninstalling transformers-4.57.6:\n",
      "      Successfully uninstalled transformers-4.57.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python-headless 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
      "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.13.0.90 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4 tokenizers-0.19.1 transformers-4.42.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "ed165ef1f5c5499fb56dd29b966e55c2",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install transformers==4.42.4\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FksK7GRMXo8"
   },
   "source": [
    "## **2. Text Classification**\n",
    "\n",
    "<img src=\"https://cdn.pixabay.com/photo/2016/01/05/17/49/good-1123013_960_720.jpg\" width=700px>\n",
    "\n",
    "LLMs can be used for text classification tasks such as sentiment analysis, topic modeling, and spam filtering.\n",
    "\n",
    "**Business Context**\n",
    "\n",
    "- In the financial industry, text classification models help categorize customer feedback, complaints, or reviews into actionable categories (e.g., fraud, product issue, satisfaction). This automates customer support workflows by prioritizing urgent issues, improving customer satisfaction, and reducing response time.\n",
    "\n",
    "- In healthcare, text classification models can be used to categorize medical reports or patient feedback, allowing hospitals to streamline diagnostics and treatments. Similarly, in social media monitoring, businesses classify posts, comments, and mentions into categories like positive, negative, or neutral sentiment to improve brand perception and customer engagement.\n",
    "\n",
    "Here's an example of sentiment analysis using the Hugging Face Transformers library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adYeTBb2FdxI"
   },
   "outputs": [],
   "source": [
    "!pip freeze >r.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482,
     "referenced_widgets": [
      "0c37ab03d8af49b7b8b1a49444b00481",
      "d4bbc308914d4abba64be8144c863c01",
      "5722c92e9dd34616b0fb44719063bee4",
      "1166edbfd7d44a8fbc62c45f44602f0d",
      "7f854d3b8fea4285a8a32ac2cda56ee9",
      "1aae3d3bf3b64dc495a8eb0ea373fa36",
      "ed113a216f4b45d988fee3fa1ca18a12",
      "48d2798168274f58995f6d3728924d8d",
      "038eb75362cf4a6b999a9c2947d2e0f0",
      "c291680eefad486f9d72786e0cd5641a",
      "179aabb5a51849f28fa7bb470e5ea1ca",
      "5efdf54872b54402a38b9db72ef95353",
      "2c8a611d1de54d6098670ff2edd79c95",
      "fd3f174ed3384a13b8f1ab87296edb20",
      "14467753be544122b2cc855bc00cca46",
      "3d4c238ce3d4427bbb614133dcf904a5",
      "ab91114df72647a999d852a51f1bb157",
      "2b137e7751294b6d926898134f38e45a",
      "81e62d3419f0495793d251941442ae8e",
      "83807d6b90e046bea6d51084bdb8047d",
      "fefd8319b6c74d3ea7173d397e1b6f07",
      "543c1e8c39504f51aedce60ea098c26e",
      "44bce44f065f46e9a9344782b75fc4b4",
      "de161c043aaa41088bbebebe4667501a",
      "7cd25ccfa59d4f7ba16c7edc808fdb8b",
      "6d70373c8ee84718ba6007ad4b1e524e",
      "8268c1745e154a4ea42713d1dabac667",
      "2f1f3d0bbe674c70ade1d9725e71378a",
      "3c88b50e8be9477298cbaa56f4834f2f",
      "785afaafa3e04b4a8bf6fc0920095354",
      "000c690295c947edac08589d75668b5c",
      "c14f0901f1784db58a7fd319a8194ce6",
      "e7743076a7f2425e82de7c42d998e031",
      "e334128c30994f62bcb587a8e28e2205",
      "cd94878199ce4e879afd1576dca03b18",
      "c34ff19af06c40739ea834ef05887691",
      "bf77941041994887a6314e0f050a6b38",
      "f1c855f005da4dc49f5ee9a0752dcf61",
      "dfe7077337f544abbe35a2f4369e2929",
      "b2fe8b7c16e24f9d8b68575878afa192",
      "8a13d3aab9eb4ff4908e74a344e1021e",
      "515215c1674e4a5781bc9ca21dd31d7b",
      "f05675b79d7640d288b72802c45bca3e",
      "946779d10f6a45f59cf2f1e550b67089",
      "89ba45bbec62410bb8dc22df8f2d571c",
      "1e4db6f55a464255a5bedc44257378d1",
      "fb827707ddaf4632829a1cd7f53547ef",
      "c06cc3f22ffd4fce82b27a353d6b0848",
      "30b3a1fcbf5b4931b1a9722835bf37f9",
      "85fddb2ae6584331904dd0de800c9d88",
      "f9497cf0eb694da997a833dd70767f5d",
      "6aae12c1431247f1acb43b2d643feb57",
      "3f07d2c0a4c14642bfe047355e08fcec",
      "93758c6723f84c07801e15724d0d2efe",
      "e4f6059167aa49fe994b714f9e7d8f79",
      "5de890b5d6f141e1b7c36fc322b70e8d",
      "eb25201abe634a1eb686cedc2665fe7c",
      "22bf00106bcb46ba991eaa477c963314",
      "7b81ad12454646e08dac70299a98231d",
      "35e6c63de2274b5098822ede83586269",
      "f8da3e9c3eea407ea8eefb0af6565bc3",
      "e4046c109fac4fb9b714d46f978cd5a2",
      "e3bc10572a0c4b2ab0b05ac77a5983df",
      "4b4aceda7c6f4f3d84fc84e23e6f5d4c",
      "b7765dfc70cf4c8bb723fe8a90d01b00",
      "1e4373bebbd64ebe9cf8b7f4a0dd854f",
      "d27bdd067b3a4b748944ff23c305925d",
      "87140872bdf54ea6b4e9097b0f383ba0",
      "f4db558982204f1bb594a6c8c10aec53",
      "39fa4b561c9c41b9b0e82ac841e87861",
      "fef40952741543619f5d6b7908fbe539",
      "9a0fd325780f4eb0a87826d340c6ae9d",
      "4d6549dbc1a14316ba2fb0ea8325432a",
      "9cd8ca778f9344feaaf3e467d576826a",
      "1a4b6bf587d3481580c3e09f2f0567d2",
      "52b28cdd126740b7ab52a3d4ff54523e",
      "d1b51f2b8fe34258bc25746d99848921"
     ]
    },
    "executionInfo": {
     "elapsed": 47515,
     "status": "ok",
     "timestamp": 1769830041800,
     "user": {
      "displayName": "Ei Mon San",
      "userId": "14838997319009891563"
     },
     "user_tz": -660
    },
    "id": "s7wPigyAFmAW",
    "outputId": "15ea7413-2cd7-40b6-8f79-2fa3c7b471b3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c37ab03d8af49b7b8b1a49444b00481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efdf54872b54402a38b9db72ef95353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bce44f065f46e9a9344782b75fc4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e334128c30994f62bcb587a8e28e2205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ba45bbec62410bb8dc22df8f2d571c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de890b5d6f141e1b7c36fc322b70e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27bdd067b3a4b748944ff23c305925d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: The quick brown fox\n",
      "Completed Text: The quick brown foxes' appearance was actually caused by their being forced to take cover during the day\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The cat sat on the\n",
      "Completed Text: The cat sat on the bed, her paw resting on his arm and he tried to play with him\n",
      "\n",
      "Prompt: When I woke up this morning\n",
      "Completed Text: When I woke up this morning to say hi to a friend and asked her to pick me up for\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_completion = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "prompt = \"The quick brown fox\"\n",
    "completed_text = text_completion(prompt, max_length=20, do_sample=True)[0]['generated_text']\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "print(f\"Completed Text: {completed_text}\\n\")\n",
    "\n",
    "prompt = \"The cat sat on the\"\n",
    "completed_text = text_completion(prompt, max_length=20, do_sample=True)[0]['generated_text']\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Completed Text: {completed_text}\\n\")\n",
    "\n",
    "prompt = \"When I woke up this morning\"\n",
    "completed_text = text_completion(prompt, max_length=20, do_sample=True)[0]['generated_text']\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Completed Text: {completed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352,
     "referenced_widgets": [
      "3435395ba8834b11ad045ee4b03bcb23",
      "f1758291545e43c6aa18fa03d58b7572",
      "52a42b52a3504bb3bc77cc865bbe39e7",
      "bf206865e01f42aa87117b3a956262dd",
      "cc1304e18d3c416da19332f3135b0776",
      "ac97aeb8eec347e1a8f18b8380dd817c",
      "f5f6429108c04250b2d913e6fb26fd78",
      "12e829f0dbbf42bf85225ffe91834354",
      "ff3ba578c5354109a28417b0daa5979a",
      "0bc4aad145644adea2ede1985976066e",
      "99972f2323834d6ab00bcb74ac0175aa",
      "e38d7625388741d5bbf2680d1263cca0",
      "055c270013ff4b058be817eca8afb068",
      "f4dd33f648ed45b28af583cb5d84e1e2",
      "8253a1646de44c859f399cd26ae1b5a2",
      "f2fd13af6cf24587b5cf009b7db24d5e",
      "3ffa8d0772bd4bd38ecc8256b36548c7",
      "1cee3515f524421c92234e3003521244",
      "0739008fc76b4285b111d4e201465a8c",
      "26e626b5efb44565b3060777804910b5",
      "c7c4ae3ca88b40389ad4f0a4e1a49779",
      "4082c607abec4d67a8e1c83f37b14231",
      "383484a630f54068825d2083f42a5b64",
      "ff215b9f4c144e208aff4688cc25b14a",
      "55aa2aa4175642e7841883610e0c813d",
      "21067d41731f474d8274c91898dd9c30",
      "7e09539422634b8dacd32bc1f8e4766a",
      "ccfa157f0d4448e5a7e825a8c6eea7ab",
      "f7e49a0c354341bea8c8190b6b69acc1",
      "1e2a88dc39504821a0d9a35b990bccb2",
      "3a63ca93c2ff443697b9a80375fa7312",
      "2e9ea943442b431cae5ac09338e1c090",
      "836c4359955a44488d5b3a672c5bec43",
      "6dc3eb0c4aeb40dfba6870e014586423",
      "136a34976eb74b4dac729ae8820b8ae9",
      "d132dfc7ede142fa81ffef30cb9179d0",
      "c7ed79e54289477fa8e145992db919e6",
      "3c55a15a1e904a538d1db3a86ea05f95",
      "64b2760e1ce1419aab6110975e48b528",
      "c0e51e80bc334b7ab89f4e7875339c9d",
      "64fc4c246b4a42d5a73a7dbd35f75787",
      "e8d8a4ba3b93403b9bb676bb061a9ab2",
      "dc057f89737c4590b1b55aaf4f491c46",
      "2e1e2653e1b443e69440a54be1288b4b"
     ]
    },
    "executionInfo": {
     "elapsed": 3751,
     "status": "ok",
     "timestamp": 1769830065291,
     "user": {
      "displayName": "Ei Mon San",
      "userId": "14838997319009891563"
     },
     "user_tz": -660
    },
    "id": "RB6vz_M1FseI",
    "outputId": "5324a5c5-5b82-4c38-ca10-02d3f2909bde"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3435395ba8834b11ad045ee4b03bcb23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38d7625388741d5bbf2680d1263cca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383484a630f54068825d2083f42a5b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc3eb0c4aeb40dfba6870e014586423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: I love this product!\n",
      "Sentiment Label: POSITIVE, Score: 0.9998855590820312\n",
      "\n",
      "Text: The plot was good but the characters were poorly developed.\n",
      "Sentiment Label: NEGATIVE, Score: 0.9994469285011292\n",
      "\n",
      "Text: The writing was brilliant, but the pacing was slow and dragged the story down.\n",
      "Sentiment Label: NEGATIVE, Score: 0.9994577765464783\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentiment analysis model\n",
    "model = pipeline('sentiment-analysis')\n",
    "\n",
    "# Analyze the sentiment of a text\n",
    "text = \"I love this product!\"\n",
    "result = model(text)[0]\n",
    "\n",
    "print(f\"\\nText: {text}\")\n",
    "# Print the sentiment label and score\n",
    "print(f\"Sentiment Label: {result['label']}, Score: {result['score']}\\n\")\n",
    "\n",
    "# Analyze the sentiment of a text\n",
    "text = \"The plot was good but the characters were poorly developed.\"\n",
    "result = model(text)[0]\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "# Print the sentiment label and score\n",
    "print(f\"Sentiment Label: {result['label']}, Score: {result['score']}\\n\")\n",
    "# Analyze the sentiment of a text\n",
    "text = \"The writing was brilliant, but the pacing was slow and dragged the story down.\"\n",
    "result = model(text)[0]\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "# Print the sentiment label and score\n",
    "print(f\"Sentiment Label: {result['label']}, Score: {result['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNpZbplpMxI1"
   },
   "source": [
    "## **3. Question Answering**\n",
    "\n",
    "<img src=\"https://cdn.pixabay.com/photo/2018/06/12/15/08/question-mark-3470783_960_720.jpg\" width=700px>\n",
    "\n",
    "LLMs can be used to answer questions based on a given text, which is of course an important part of Conversational AI experiences.\n",
    "\n",
    "**Business Context**\n",
    "\n",
    "- LLM-powered question answering is widely used in customer service through chatbots that provide instant responses to user inquiries based on large sets of FAQs, product descriptions, and user manuals. In e-commerce, QA systems help users quickly find product information, warranty details, and shipping options, resulting in improved customer experience.\n",
    "\n",
    "- In the legal and insurance industries, QA systems are used to extract relevant information from lengthy policy documents, contracts, or regulatory guidelines. This can significantly reduce the time required for employees to retrieve critical information, improving operational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435,
     "referenced_widgets": [
      "847a62827dde41568cbe04f280da6f10",
      "764c67f80b094b35972633dac2e1db55",
      "4ff827ff47c342648255633b11c13060",
      "1e354bf7898a4df381e2a067219e5f29",
      "7990ee9ee8de4f88ab10ff29d33bba05",
      "0d8afb6ab6e94472bb5da33f584ff094",
      "d2af2d129e7647658a858ce0c036c07e",
      "b4278935df2e45cc883ee2472242f6d0",
      "b79ad1092021440a836dd7a59a1476ed",
      "5ecb324551bb431ca76940e8eaafeb1a",
      "ed8c9763b9954b9abd80137e3fb82750",
      "a8a0a1cd3be1403db2c86f3a576a7098",
      "54e61be3f5b1449dbc3e785344e623d3",
      "c61ed2602bc040a78ba5e2954e872038",
      "b6658b1372bd42b19bc8eeda14044ce6",
      "10c9626a0c67440fa63a354f0cbac298",
      "811dadb260c94433b478422e6d8efb37",
      "17013b23f36d4a20beb7ae20b2e652fb",
      "617e144ea1d14c3b9eb31d10a200b01c",
      "7597c068fc2b4d22b62ae499fa744f34",
      "4280beee48ef46c0bdf1bbc0e6bd646e",
      "24dc882c416f4c288a6f728ceb05bb78",
      "c24ebf542b964c438678f7f7d61f2cd5",
      "ad0b3fb526874a82b97a6d7543f6baf2",
      "a94a5eb823d845ed9c2051ac27479cff",
      "bc332da993cb4a2fad64859ec3bc8b2d",
      "b9e5d89fdd31412fb261e83b01937f69",
      "50a4f63bc0b448fcbfa20330941e0ebe",
      "c42c4cd3c97c4142a707d6946b5a0ae4",
      "564ad07b9ca6461ebbdbcef4447cc95e",
      "b3604c3fb27e44e1b29e152f11239c82",
      "ae289fea58994eb0bfb09b15cb5fa499",
      "3955cb3cfaf046fbb5429b833fec3780",
      "c429ccbe7bee461ab7efbb46240859b2",
      "3c73f67939434a0f899b974a698c4ccc",
      "377da12913df4b45b6fc0d846fa0bbad",
      "89bc3234c2254aeb854018e4d2c29b9c",
      "b693f71020b946a1b60490eea9a1d22b",
      "96135f808d9e498fa45886386d448d02",
      "7df88739300f4249a3a84ed68600554f",
      "532baced82574f90a5f50ed28aa34995",
      "f40e97125ad649a7bd2fc2832f8dde7c",
      "f2263100565648e78d20317e3bb2f81d",
      "c6b33d74a67f4ed7aba5777656cabfd9",
      "80285201ebc6444aab8a727b94b6e384",
      "25a972687fd640baac940712f70c2ce5",
      "be6903d112df43689f2192344495e9f7",
      "8a436508110a490896c486aec9c1b4b3",
      "8248a778243847129cd04cf3e5da8c88",
      "701806d8fb494a7a9fc4d74646cefe1e",
      "783033c5cc55407e8df04add6a6826f0",
      "576edc88cef84647bf65a644b6e69091",
      "bcd4c36f2a9d4d6293679284d5dd7bd4",
      "352c056fe2db48718ab9d76858207a09",
      "ba704537b4f244999b5e7f03fba0f7e2"
     ]
    },
    "executionInfo": {
     "elapsed": 6876,
     "status": "ok",
     "timestamp": 1769830114516,
     "user": {
      "displayName": "Ei Mon San",
      "userId": "14838997319009891563"
     },
     "user_tz": -660
    },
    "id": "rrckVHZuGB6h",
    "outputId": "3d2d4417-bac6-45e0-8e42-21a5b7f9da10"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847a62827dde41568cbe04f280da6f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a0a1cd3be1403db2c86f3a576a7098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24ebf542b964c438678f7f7d61f2cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c429ccbe7bee461ab7efbb46240859b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80285201ebc6444aab8a727b94b6e384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text (or Context): The Mona Lisa is a 16th century portrait painted by Leonardo da Vinci.\n",
      "Question: Who painted the Mona Lisa?\n",
      "Answer by Model: Leonardo da Vinci\n",
      "\n",
      "Text (or Context): The band members played their favourite songs for the audience during the concert last night. Despite the power outage, the band managed to complete their performance.\n",
      "Question: What did the band do during the concert?\n",
      "Answer by Model: played their favourite songs for the audience\n",
      "\n",
      "Text (or Context): The genetic makeup of a living organism is determined by its DNA, which is made up of nucleotides arranged in a specific sequence. This sequence provides instructions for the organism's physical and functional characteristics.\n",
      "Question: What is the function of an organism's DNA?\n",
      "Answer by Model: physical and functional characteristics\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the question answering model\n",
    "model = pipeline('question-answering')\n",
    "\n",
    "# Answer a question based on a text\n",
    "text = \"The Mona Lisa is a 16th century portrait painted by Leonardo da Vinci.\"\n",
    "question = \"Who painted the Mona Lisa?\"\n",
    "result = model(question=question, context=text)\n",
    "\n",
    "print(f\"\\nText (or Context): {text}\")\n",
    "print(f\"Question: {question}\")\n",
    "# Print the answer\n",
    "print(f\"Answer by Model: {result['answer']}\\n\")\n",
    "\n",
    "\n",
    "# Answer a question based on a text\n",
    "text = \"The band members played their favourite songs for the audience during \\\n",
    "the concert last night. Despite the power outage, the band managed to complete \\\n",
    "their performance.\"\n",
    "question = \"What did the band do during the concert?\"\n",
    "result = model(question=question, context=text)\n",
    "\n",
    "print(f\"Text (or Context): {text}\")\n",
    "print(f\"Question: {question}\")\n",
    "# Print the answer\n",
    "print(f\"Answer by Model: {result['answer']}\\n\")\n",
    "\n",
    "# Answer a question based on a text\n",
    "text = \"The genetic makeup of a living organism is determined by its DNA, which \\\n",
    "is made up of nucleotides arranged in a specific sequence. This sequence \\\n",
    "provides instructions for the organism's physical and functional characteristics.\"\n",
    "question = \"What is the function of an organism's DNA?\"\n",
    "result = model(question=question, context=text)\n",
    "\n",
    "print(f\"Text (or Context): {text}\")\n",
    "print(f\"Question: {question}\")\n",
    "# Print the answer\n",
    "print(f\"Answer by Model: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCwdrpryNA7X"
   },
   "source": [
    "## **4. Text Generation**\n",
    "\n",
    "<img src=\"https://cdn.pixabay.com/photo/2015/07/17/22/43/student-849825_960_720.jpg\" width=700px>\n",
    "\n",
    "\n",
    "LLMs can also be used for Text Generation - to generate longer paragraphs of text based on a given prompt (as opposed to just Text Completion which may be for a shorter expected output).\n",
    "\n",
    "**Business Context**\n",
    "\n",
    "- Text generation models are transforming the way businesses create content. In marketing, LLMs generate personalized email campaigns, social media posts, and product descriptions, allowing companies to target specific audiences at scale. In news and journalism, LLMs can automatically generate reports or summaries based on data or articles, speeding up content creation and reducing manual labor.\n",
    "\n",
    "- Text generation is also used in education, where it can generate learning materials, quizzes, and training guides based on specific topics, allowing institutions to scale up content delivery for online learning platforms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25004,
     "status": "ok",
     "timestamp": 1769830157612,
     "user": {
      "displayName": "Ei Mon San",
      "userId": "14838997319009891563"
     },
     "user_tz": -660
    },
    "id": "jpz66W0tGHQA",
    "outputId": "9ca469eb-94a5-4a60-fab8-5bea07c71bdf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt:\n",
      "Once upon a time,\n",
      "Generated Text:\n",
      "Once upon a time, a black market is the only legitimate form of competition that exists. Even then, you might expect to discover those \"good deals\" and see that it actually pays you less in taxes. So, if you're worried about paying\n",
      "\n",
      "-----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt:\n",
      "The airplane was flying at 30,000 feet when suddenly it\n",
      "Generated Text:\n",
      "The airplane was flying at 30,000 feet when suddenly it crashed on the runway. The pilot of the plane reportedly tried to go up the side to try to kill the pilot but got his gun and shot him multiple times.\"\n",
      "\n",
      "He added,\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "Prompt:\n",
      "The cat slept on the sofa for two hours and then suddenly woke up and jumped on the\n",
      "Generated Text:\n",
      "The cat slept on the sofa for two hours and then suddenly woke up and jumped on the sofa. He was crying with glee and started talking to himself again.\n",
      "\n",
      "The cat had gone to pick his own leg up and he also decided how\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the text prompt\n",
    "prompt = \"Once upon a time,\"\n",
    "\n",
    "# Generate text based on the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "sample_output = model.generate(input_ids, do_sample=True, max_length=50)\n",
    "\n",
    "# Decode the generated text and print it\n",
    "generated_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\nPrompt:\\n{prompt}\")\n",
    "print(f\"Generated Text:\\n{generated_text}\\n\")\n",
    "\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "# Set the text prompt\n",
    "prompt = \"The airplane was flying at 30,000 feet when suddenly it\"\n",
    "\n",
    "# Generate text based on the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "sample_output = model.generate(input_ids, do_sample=True, max_length=50)\n",
    "\n",
    "# Decode the generated text and print it\n",
    "generated_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\nPrompt:\\n{prompt}\")\n",
    "print(f\"Generated Text:\\n{generated_text}\\n\")\n",
    "\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "# Set the text prompt\n",
    "prompt = \"The cat slept on the sofa for two hours and then suddenly woke up \\\n",
    "and jumped on the\"\n",
    "\n",
    "# Generate text based on the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "sample_output = model.generate(input_ids, do_sample=True, max_length=50)\n",
    "\n",
    "# Decode the generated text and print it\n",
    "generated_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(f\"Generated Text:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecekeyYFNJag"
   },
   "source": [
    "## **5. Text Summarization**\n",
    "\n",
    "<img src=\"https://cdn.pixabay.com/photo/2017/10/24/21/25/arrow-2886223_960_720.jpg\" width=700px>\n",
    "\n",
    "\n",
    "LLMs can also be used to summarize longer texts into shorter ones - a key use case of NLP that is common to various industries and businesses.\n",
    "\n",
    "**Business Context**\n",
    "\n",
    "- In industries like finance and consulting, executives are often bombarded with lengthy reports and documents. Text summarization tools powered by LLMs extract the key points from these documents, enabling faster decision-making. Summarization models are especially useful in legal settings, where they condense legal contracts or case histories, allowing attorneys to focus on the most relevant information without reading through voluminous documentation.\n",
    "\n",
    "- In healthcare, summarization tools help distill patient records and research papers into concise summaries, improving the workflow for doctors, researchers, and medical professionals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484,
     "referenced_widgets": [
      "16a312c421ea48ce80c8834b973d5194",
      "cdb91148287448e0be23ab80726403a0",
      "3cf40bc0937e4d8599a096367dbc57ee",
      "f86b3a05f5b14189a1e4654085c3a1ba",
      "7c7933d5262a4f33b81aac9aa62efe1f",
      "0ab33f7c6dbd40dca92b5bea28096a32",
      "545d78d4013e4b6fab7123a38271e4ae",
      "d116287d516d4ce1bb7faed55f2adc05",
      "24033e0e80e541aaafd664a96dff4f61",
      "bd0cf89e64d34005a3dd4b7bacd34f4f",
      "9cad6cd31951423286a9a63ce2dc7376",
      "0cfdd2bf9f844fa1bf17bee179e35f03",
      "d074b3f7208748c9b50dc6f7b2cfbce6",
      "5497c88f01fc4864acf6df55859da8b6",
      "8413c015251c46579cc04edf0e2b381b",
      "db73f5fe7559400382e497b5fd5fbbd4",
      "026af2d01abc4836bf580ad31c4a6762",
      "febdd99deee44206a979c42d133ab8ed",
      "ffacbfcb2acd4488b5016d80bee6caec",
      "8cd6e222195b490c9f0509b2dfa8e5ea",
      "76f4056354cd476bbc216c4388e55676",
      "dc3005fd688e452abe699734622ac4ed",
      "abb706df3d8d47208116edb6e497edbe",
      "87ef28a2f0bd47858fb97b5ca4ff4c21",
      "eb805b37ba8f497fba22f15708a610ac",
      "b116cbabbb0e49018ac8c61b5914fa71",
      "d87064c7d2384f3b8ca2b40786a387fe",
      "5419fec3fd2c48548ff8cb78c7f941ce",
      "12b20690de6b45cc99ea36ea1797ed0a",
      "919f8656e297466fba288eaf88bdb084",
      "e752ad0876c945a0805832093e11bf85",
      "4f67efe2eebf4edfa97f36ecd884eb6f",
      "fc73f3168adb464f9893c6ae66575ea0",
      "6a4ed3b4a36047ad8b8540e369376d1c",
      "cd737e7669284a68bc626a3040276992",
      "f2a86c2738d94fa8b7b0d16b1c10a4cd",
      "f9299948cca8401fa8ba341987a1016e",
      "f98f86ead1a14e86b6590e6aa0bec574",
      "c2665f2775f7425ca0a78a78bf28fb5f",
      "3f2fbb873a9b4914b6cdd2477ddd6c96",
      "77369ef02fc040299460a511c6177f4e",
      "583388d832c64f469f6e002a678127d5",
      "a02e28841c494c0cafde8139627d5769",
      "9d47c86e953f4739bff74d108ce3b8f8",
      "196d5303817246079dd48a04605afa98",
      "3fa2a66e347a4f1f926bba73ff5a89d1",
      "a71fca79a2304a07a4dea362aacece53",
      "89b1b84f014145bcbf7a80c0efb73633",
      "a26c4b258dd04ce3b4a807ac46cac850",
      "96c98375fec342ea991b1979c9821017",
      "cd1a3718ea9a44f898edbfbc0059158b",
      "19e3af09757342bfa46ff990ba8a2650",
      "07e9318a16de453b98c07c0f26ae6205",
      "4bc804a02cb74e20836cad56deca61e3",
      "1ebffa9fbda24ca0b4718775d205cc0f",
      "270c446041df407dbec0a1819c42d618",
      "69deea22e467407a923a8ec2be3c9124",
      "f05a338545bc4eaebb6bc4b3733b0a1f",
      "ea3d65d00d044e1289c6d5cc2427dba0",
      "fda56d0fde874c33b2aa17c357f9b4cf",
      "8cb13115832b440993d8861be5f7deb7",
      "b758f2061b3e45c193475e3c1bcb8cdc",
      "c92710cbcbee44ecab165fc31713419c",
      "0e38973a4fec42cf86e5070643eadabf",
      "4eeb746671f54da4840a37e272e54648",
      "b4ad424a7cff481f83874558987058bf"
     ]
    },
    "executionInfo": {
     "elapsed": 21462,
     "status": "ok",
     "timestamp": 1769830181717,
     "user": {
      "displayName": "Ei Mon San",
      "userId": "14838997319009891563"
     },
     "user_tz": -660
    },
    "id": "3S96XBHOGNKi",
    "outputId": "429c96b6-ac74-42c8-81f8-14fc5911d0ea"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a312c421ea48ce80c8834b973d5194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfdd2bf9f844fa1bf17bee179e35f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb706df3d8d47208116edb6e497edbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4ed3b4a36047ad8b8540e369376d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196d5303817246079dd48a04605afa98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270c446041df407dbec0a1819c42d618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text:\n",
      "The rise of artificial intelligence is set to transform our world in ways  we can barely imagine. AI will revolutionize the way we work, the way we learn, and  the way we live. With the power of AI, we will be able to solve some of the world's  most pressing problems, from climate change to poverty to disease.\n",
      "Summarized Text:\n",
      "the rise of artificial intelligence is set to transform our world in ways we can barely imagine . we will be able to solve some of the world's most pressing problems, from climate change to poverty .\n",
      "\n",
      "Original Text:\n",
      "The theory of relativity, proposed by Albert Einstein, revolutionized our understanding of space, time, and gravity. It has become one of the most famous scientific theories of all time, and has been the subject of countless studies and debates. Despite its wide acceptance and numerous applications, there are still many aspects of the theory that remain unresolved or unexplained, leading some scientists to propose new theories or modifications to the original theory.\n",
      "Summarized Text:\n",
      "the theory of relativity, proposed by Albert Einstein, revolutionized our understanding of space, time, and gravity . there are still many aspects of the theory that remain unresolved or unexplained .\n",
      "\n",
      "Original Text:\n",
      "The novel 'To Kill a Mockingbird' by Harper Lee is a classic of American literature. It is set in the 1930s in a small town in Alabama and tells the story of a young girl named Scout Finch and her family. The novel explores themes of racism, injustice, and social inequality, and has been lauded for its honest and moving portrayal of these issues. It has won numerous awards and has been adapted into multiple films and plays.\n",
      "Summarized Text:\n",
      "the novel explores themes of racism, injustice, and social inequality . it has been lauded for its honest portrayal of these issues .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the T5 summarization model\n",
    "model = pipeline('summarization', model='t5-small', tokenizer='t5-small')\n",
    "\n",
    "# Summarize a longer text\n",
    "text = \"The rise of artificial intelligence is set to transform our world in ways \\\n",
    " we can barely imagine. AI will revolutionize the way we work, the way we learn, and \\\n",
    " the way we live. With the power of AI, we will be able to solve some of the world's \\\n",
    " most pressing problems, from climate change to poverty to disease.\"\n",
    "\n",
    "summary = model(text, max_length=50, min_length=10, do_sample=False)\n",
    "\n",
    "print(f\"\\nOriginal Text:\\n{text}\")\n",
    "\n",
    "# Print the summarized text\n",
    "print(f\"Summarized Text:\\n{summary[0]['summary_text']}\\n\")\n",
    "\n",
    "# Summarize a longer text\n",
    "text = \"The theory of relativity, proposed by Albert Einstein, revolutionized \\\n",
    "our understanding of space, time, and gravity. It has become one of the most \\\n",
    "famous scientific theories of all time, and has been the subject of countless \\\n",
    "studies and debates. Despite its wide acceptance and numerous applications, \\\n",
    "there are still many aspects of the theory that remain unresolved or \\\n",
    "unexplained, leading some scientists to propose new theories or modifications \\\n",
    "to the original theory.\"\n",
    "\n",
    "summary = model(text, max_length=50, min_length=10, do_sample=False)\n",
    "\n",
    "print(f\"Original Text:\\n{text}\")\n",
    "\n",
    "# Print the summarized text\n",
    "print(f\"Summarized Text:\\n{summary[0]['summary_text']}\\n\")\n",
    "\n",
    "# Summarize a longer text\n",
    "text = \"The novel 'To Kill a Mockingbird' by Harper Lee is a classic of \\\n",
    "American literature. It is set in the 1930s in a small town in Alabama and \\\n",
    "tells the story of a young girl named Scout Finch and her family. The novel \\\n",
    "explores themes of racism, injustice, and social inequality, and has been \\\n",
    "lauded for its honest and moving portrayal of these issues. It has won numerous \\\n",
    "awards and has been adapted into multiple films and plays.\"\n",
    "\n",
    "summary = model(text, max_length=50, min_length=10, do_sample=False)\n",
    "\n",
    "print(f\"Original Text:\\n{text}\")\n",
    "\n",
    "# Print the summarized text\n",
    "print(f\"Summarized Text:\\n{summary[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzFppdlUNW2s"
   },
   "source": [
    "## **6. Named Entity Recognition (NER)**\n",
    "\n",
    "<img src=\"https://cdn.pixabay.com/photo/2017/10/14/01/37/feedback-2849601_960_720.jpg\" width=700px>\n",
    "\n",
    "Large language models can also be used for NER - to identify and extract named entities from text, such as person names, locations, and organizations.\n",
    "\n",
    "**Business Context**\n",
    "\n",
    "- Named Entity Recognition is crucial for automating data extraction processes. In finance, NER models can identify and extract important entities like names, organizations, amounts, and dates from invoices, receipts, or contracts. This accelerates the process of auditing and financial analysis by ensuring the relevant information is extracted from unstructured documents.\n",
    "\n",
    "- In customer support and CRM systems, NER is used to identify important details from customer emails or feedback (e.g., product names, locations, and times). This allows businesses to automatically categorize and prioritize customer inquiries, improving service efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539,
     "referenced_widgets": [
      "90a7397f2e8946e5853ef095ced27b95",
      "75064d8ab16a410f9a0710af57c04925",
      "bf06a5fcc7ba4aea8ca999593ef6d02e",
      "b01cc550aae04c4dab3285c03b312721",
      "27cee90891b843dbb5f310803c902d5e",
      "1632ae9a4ed344c5a891b027689b823b",
      "15bc1ac6657c48e6ab8d5d471b431479",
      "2c7dc8127e564a20ac3402046f0ffe9e",
      "f70cc48226284f5899d460cbc56f20c6",
      "8ce55f26200844dc91238a01d00489bd",
      "d43acc711ada42ceb1dff9549041563d",
      "f38db5496b854a798d500197c7e8b2bf",
      "328a21e6123b4250a13816e92474eab9",
      "9bcd3ee58da54e059543a75bfdc6244a",
      "3a0f2db075834e7d95be7475094a1502",
      "32d7ea54def041aeb15c07c7fae6a220",
      "7890ce47af2e45f7a6b259a68c73fb3f",
      "520bb389dc7043ab9e0b06a728dcee23",
      "cb9775ad594d4b9dabaaae38d8e84a51",
      "23cf3f6abe604fecb10f6915f80de875",
      "a75ff106fc254f54ac61cbeeb8b21df0",
      "1fefaeaff3644aea90f998bf3e17aef4",
      "62e6c0e32468431db6a4936a25b9edc3",
      "daae8cc51089433296e47868ae0799ca",
      "9575b9d797564d80ba8f472e7b6c3e85",
      "068c5c6e4a2b42158e0abe46c7b7d9e1",
      "0499cad789f8448bbaf1b1b6f2280316",
      "e7674ecd2e5e45cab1c8422d30eda6bf",
      "48e93bce88164d22baa6113b4f11e185",
      "91fd661398c640fb9c58a8203e6e7537",
      "fdd43d933d594a77969e2ea192e69555",
      "b14be379dc5c4d82b4fce08a1b67530b",
      "abfd922daba24dfd97098b2e071d1a25",
      "a07df25a01744f1faeb3574bdcdb43d1",
      "9abdca55b7594184b0232a136f255e93",
      "21f9e4b8cda2466eaca9ddc400ba1c82",
      "82663811a85a4519b21636626c17b398",
      "81d2df0ad1384d3eb855ab0af5936792",
      "51cc1e88ed7e4786a1742afc77657840",
      "5502f901762f44ac8757b6b28b8fa514",
      "4d04c1c303654eb1ae7bc7d3e037bdce",
      "9f2082db78894dd7a05ae5df768c15fc",
      "3a3bee6142df499f94972260692160ed",
      "535fcbf6ac954f13845de429d0a66bef"
     ]
    },
    "executionInfo": {
     "elapsed": 45085,
     "status": "ok",
     "timestamp": 1769848889333,
     "user": {
      "displayName": "Ei Mon San",
      "userId": "14838997319009891563"
     },
     "user_tz": -660
    },
    "id": "JXlIOdpiGYnD",
    "outputId": "824d9fda-114c-490f-a84c-6d27ecca806c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a7397f2e8946e5853ef095ced27b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38db5496b854a798d500197c7e8b2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e6c0e32468431db6a4936a25b9edc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07df25a01744f1faeb3574bdcdb43d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk: PER\n",
      "Tesla: ORG\n",
      "SpaceX: ORG\n",
      "California: LOC\n",
      "The Count of Monte Cristo: MISC\n",
      "Alexandre Dumas: PER\n",
      "Edmond Dantès: PER\n",
      "Château d ' If: LOC\n",
      "A: PER\n",
      "##é Faria: PER\n",
      "Melbourne: LOC\n",
      "Apple: ORG\n",
      "Tim Cook: PER\n",
      "John Smith: PER\n",
      "San Francisco: LOC\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the named entity recognition model\n",
    "model = pipeline('ner', grouped_entities=True)\n",
    "\n",
    "# Identify named entities in a text\n",
    "text = \"Elon Musk is the CEO of Tesla and SpaceX, and lives in California.\"\n",
    "entities = model(text)\n",
    "\n",
    "# Print the named entities and their labels\n",
    "for entity in entities:\n",
    "    print(f\"{entity['word']}: {entity['entity_group']}\")\n",
    "\n",
    "# Identify named entities in a text\n",
    "text = \"I recently read a book called 'The Count of Monte Cristo' by Alexandre \\\n",
    "Dumas. The main character, Edmond Dantès, was imprisoned in Château d'If for a \\\n",
    "crime he didn't commit. He escaped with the help of a fellow prisoner named Abbé Faria.\"\n",
    "entities = model(text)\n",
    "\n",
    "# Print the named entities and their labels\n",
    "for entity in entities:\n",
    "    print(f\"{entity['word']}: {entity['entity_group']}\")\n",
    "\n",
    "# Identify named entities in a text\n",
    "text = \"I live in the city of Melbourne and work for a company called Apple. My \\\n",
    "boss is Tim Cook and my colleague's name is John Smith. Yesterday, I had lunch \\\n",
    "at a restaurant called San Francisco in the city center.\"\n",
    "entities = model(text)\n",
    "\n",
    "# Print the named entities and their labels\n",
    "for entity in entities:\n",
    "    print(f\"{entity['word']}: {entity['entity_group']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmpOFGxFNj9y"
   },
   "source": [
    "## **7. Language Translation**\n",
    "\n",
    "<img src=\"https://cdn.pixabay.com/photo/2022/08/16/06/30/different-language-7389469_960_720.jpg\" width=700px>\n",
    "\n",
    "Another long-standing use case of NLP that is being solved by LLMs is Language Translation.\n",
    "\n",
    "**Business Context**\n",
    "\n",
    "- Language translation models enable businesses to break down language barriers and expand into global markets. In content localization, global media companies use translation models to translate movies, articles, and websites into multiple languages, enhancing accessibility for international audiences and boosting revenue streams.\n",
    "\n",
    "- In diplomatic services and international relations, translation models assist government agencies and organizations in translating official documents and communication, ensuring that the nuances of each language are preserved in policy-making and diplomatic relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466,
     "referenced_widgets": [
      "67a283b5fa2043deaadab666025148d9",
      "c9f7535945f5499e95e30207e01678a4",
      "cc00e816793a4dc3a02bed6630867e12",
      "8ef76033ba2b4b0f9b8f68864983cb98",
      "018d342d1e2d4bf2ae5061696fe7cee6",
      "2fd3ada2d8d6440988be9c034c3e4935",
      "c794be319c7448f590b532762056f2ff",
      "52cf33c1fbcf4dca9436da0a3eb8ea35",
      "1fb9398aa39f4328972ffc478d56f5df",
      "1f9026833cfa421c985b0e7b39c47394",
      "55cb0dd280f243818d785065cf7b9ee2",
      "075f5e0a440942c4bfb214c869e484b5",
      "452c3272f700439db175f18481f45da5",
      "d421eb6b4b7f4683a36e05c979f01a61",
      "4b7d58cc0cc64a79b2dfe5a741bf2084",
      "ae6c0779c25c41bea1fe57f28f2011c5",
      "0bfa5d6390fb41259640b2c081837b3e",
      "a53d3e883380447797bc433b6b889390",
      "817b8caaa2ae4fc3a1b505978fe417d2",
      "eb8b02fd31ff4e46bed68398ab61c106",
      "8eaad2f54c534c4bba005d0135194521",
      "88664afe29db4c90a6a0cdb03cfd7967",
      "0a60a6f588d8466681682b90b8b0eb78",
      "d82815051d354a38b04f54215a43c2ac",
      "d08616b3d26b4e0592b1aaa4ca5e9b23",
      "d624ac3b09db407da3650f3d162b9d5a",
      "ade33ac33e5f4645a3f09c60618e3854",
      "aea04bff108e45e4b91e8595d9706fc8",
      "f758db7ca01542da80d6d82d58c3dda7",
      "1ea343905825421b8fd9f967231b3ece",
      "78ff162373924d7f821f952322010fbb",
      "fc8aad01755d4133b43c8a9ee25fed8f",
      "df0d057511824020bf04797450f60b72",
      "7c9f0990ed8a4009bb6d624f34e5d627",
      "091c446f7db2404f8348bf4153f63580",
      "a50c45adeab34a828096859f0c230d58",
      "0b50c3c9ecfb4c3da7c65ed7873faecc",
      "0d3d1cdb495d4d8ba5d63c9cd6c49458",
      "94805b5cfb09476194532227ff0750cf",
      "d74ffc8639594891a0731c106f9b8094",
      "8811cbda62db4647b97d1bf1a7d7719e",
      "f758d4a4aca6477791ff4b8d5f027f39",
      "9ad62b32807b43178674406a405e6576",
      "381d47d60f62440abe046a70128252a3",
      "997e817c7c9e4eda9640c0e06f9efa17",
      "d9449d13c252489aa2ac64c463f3d8e7",
      "d034314c273849f083be2b84872ab325",
      "34c87dd219ed4fd5aa1741c031a57e1b",
      "5b3661ba802d4e4aab81399cf495143b",
      "0b2da5a377bd4d048a0086adea198e53",
      "c6542e71cc394962823e052c7ac681bd",
      "29ef1286ece64e0aacc05d2ad5bd330d",
      "3b7723b02ba041b8936ca56fa932799c",
      "2389dacabff04c31b18422207d75c1d2",
      "ae83a00215144892a583e7d0ef7e83b0"
     ]
    },
    "executionInfo": {
     "elapsed": 35924,
     "status": "ok",
     "timestamp": 1769830256513,
     "user": {
      "displayName": "Ei Mon San",
      "userId": "14838997319009891563"
     },
     "user_tz": -660
    },
    "id": "IZ21foFWGYuC",
    "outputId": "0d575053-b60f-4106-ff2d-bdd2e5d9794f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a283b5fa2043deaadab666025148d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075f5e0a440942c4bfb214c869e484b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a60a6f588d8466681682b90b8b0eb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9f0990ed8a4009bb6d624f34e5d627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997e817c7c9e4eda9640c0e06f9efa17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded English to French Translation Model!\n",
      "\n",
      "Original Text: The quick brown fox jumped over the lazy dog.\n",
      "Translated Text in French: Le renard brun rapide a sauté au-dessus du chien laxiste.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text: The blackboard is black\n",
      "Translated Text in French: Le tableau noir est noir.\n",
      "\n",
      "Successfully loaded English to German Translation Model!\n",
      "\n",
      "Original Text: The sun is shining and the birds are chirping\n",
      "Translated Text in German: Die Sonne scheint und die Vögel chirren\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the translation model\n",
    "model = pipeline('translation_en_to_fr')\n",
    "\n",
    "print(\"Successfully loaded English to French Translation Model!\")\n",
    "\n",
    "# Translate an English text to French\n",
    "text = \"The quick brown fox jumped over the lazy dog.\"\n",
    "translation = model(text, max_length=40)\n",
    "\n",
    "print(f'\\nOriginal Text: {text}')\n",
    "\n",
    "# Print the translated text\n",
    "print(f'Translated Text in French: {translation[0][\"translation_text\"]}\\n')\n",
    "\n",
    "# Translate an English text to French\n",
    "text = \"The blackboard is black\"\n",
    "translation = model(text, max_length=40)\n",
    "\n",
    "print(f'\\nOriginal Text: {text}')\n",
    "\n",
    "# Print the translated text\n",
    "print(f'Translated Text in French: {translation[0][\"translation_text\"]}\\n')\n",
    "\n",
    "# Load the translation model\n",
    "model = pipeline('translation_en_to_de')\n",
    "\n",
    "print(\"Successfully loaded English to German Translation Model!\")\n",
    "\n",
    "# Translate an English text to German\n",
    "text = \"The sun is shining and the birds are chirping\"\n",
    "translation = model(text, max_length=40)\n",
    "\n",
    "print(f'\\nOriginal Text: {text}')\n",
    "\n",
    "# Print the translated text\n",
    "print(f'Translated Text in German: {translation[0][\"translation_text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2831,
     "status": "ok",
     "timestamp": 1769850172174,
     "user": {
      "displayName": "Ei Mon San",
      "userId": "14838997319009891563"
     },
     "user_tz": -660
    },
    "id": "yAN5N1sQN_LP",
    "outputId": "e2cf55e2-c3b9-47a1-9740-97e2c4565b1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "error",
     "timestamp": 1769850173597,
     "user": {
      "displayName": "Ei Mon San",
      "userId": "14838997319009891563"
     },
     "user_tz": -660
    },
    "id": "Nh6JOnk0ORGd",
    "outputId": "0d87427d-a6b4-4467-89a5-b98f2c614844"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/LangChain_Dcoument_Q&A_.ipynb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nbformat/__init__.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(fp, as_version, capture_validation_error, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1272117170.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/LangChain_Dcoument_Q&A_.ipynb\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"widgets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"widgets_state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nbformat/__init__.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(fp, as_version, capture_validation_error, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: PTH123\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_validation_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/LangChain_Dcoument_Q&A_.ipynb'"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "path = \"/content/drive/MyDrive/Colab Notebooks/LangChain_Dcoument_Q&A_.ipynb\"\n",
    "nb = nbformat.read(path, as_version=4)\n",
    "nb.metadata.pop(\"widgets\", None)\n",
    "nb.metadata.pop(\"widgets_state\", None)\n",
    "nb.metadata.pop(\"widgets_state\", None)\n",
    "for cell in nb.cells:\n",
    "  cell.metadata.pop(\"widgets\", None)\n",
    "nbformat.write(nb, path)\n",
    "print(\"Cleaned notebook metadata for Github:\", path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPwjxqzb2BAoxxSV1aRbvYT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
